# Sentence-Embeddings-Using-N-Gram-Features-and-Contrastive-Learning-for-Multilingual-Datasets

This project explores a novel approach to generating sentence embeddings by leveraging **N-gram features** combined with **contrastive learning**. It aims to improve semantic representation across **multilingual datasets**, especially for low-resource languages.

---

## ğŸš€ Features

- Sentence embeddings using character and word N-grams  
- Contrastive learning framework to align similar sentences  
- Works across multilingual and domain-diverse datasets  
- Evaluation using cosine similarity and clustering visualization

---

## ğŸ“ Datasets Used

- `Harry Potter and The Half-Blood Prince.txt` â€“ Fictional narrative
- `bible.txt` â€“ Multilingual structured text
- `Dataset25.txt` â€“ Custom multilingual dataset

---

## ğŸ› ï¸ Setup & Usage

1. **Clone the repository**
   ```bash
   git clone https://github.com/MS134340/Sentence-Embeddings-Using-N-Gram-Features-and-Contrastive-Learning-for-Multilingual-Datasets.git
   cd Sentence-Embeddings-Using-N-Gram-Features-and-Contrastive-Learning-for-Multilingual-Datasets

## Install required libraries
pip install numpy pandas scikit-learn matplotlib seaborn

## Run the notebook
Open and execute the .ipynb file in Jupyter or Google Colab.

## ğŸ“Š Results Summary
Outperformed simple TF-IDF and averaging methods.
Embeddings better captured sentence-level semantic similarity.
Contrastive loss improved clustering of similar meanings.

ğŸ§  Techniques Used
N-gram Feature Engineering
Sentence Vector Aggregation
Supervised Contrastive Loss
Cosine Similarity Evaluation

ğŸ“‚ Project Structure
â”œâ”€â”€ Dataset/
â”‚   â”œâ”€â”€ Dataset25.txt
â”‚   â”œâ”€â”€ bible.txt
â”‚   â””â”€â”€ Harry Potter and The Half-Blood Prince.txt
â”œâ”€â”€ Sentence-Embeddings-Using-N-Gram-Features.ipynb
â””â”€â”€ README.md
